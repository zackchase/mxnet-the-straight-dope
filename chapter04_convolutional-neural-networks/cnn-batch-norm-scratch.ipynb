{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization from scratch\n",
    "\n",
    "When you train a linear model, you update the weights\n",
    "in order to optimize some objective.\n",
    "And for the linear model, \n",
    "the distribution of the inputs stays the same throughout training.\n",
    "So all we have to worry about is how to map \n",
    "from these well-behaved inputs to some appropriate outputs.\n",
    "But if we focus on some layer in the middle of a deep neural network,\n",
    "for example the third,\n",
    "things look a bit different. \n",
    "After each training iteration, \n",
    "we update the weights in all the layers, including the first and the second.\n",
    "That means that over the course of training,\n",
    "as the weights for the first two layers are learned,\n",
    "the inputs to the third layer might look dramatically different than they did at the beginning.\n",
    "For starters, they might take values on a scale orders of magnitudes different from when we started training.\n",
    "And this shift in feature scale might have serious implications, say for the ideal learning rate at each time. \n",
    "\n",
    "To explain, let us consider the Taylor's expansion for the objective function $f$ with respect to the updated parameter $\\mathbf{w}$, such as $f(\\mathbf{w} - \\eta \\nabla f(\\mathbf{w}))$. Coefficients of those higher-order terms with respect to the learning rate $\\eta$ may be so large in scale (usually due to many layers) that these terms cannot be ignored. However, the effect of common lower-order optimization algorithms, such as gradient descent, in iteratively reducing the objective function is based on an important assumption: all those higher-order terms with respect to the learning rate in the aforementioned Taylor's expansion are ignored.\n",
    "\n",
    "\n",
    "Motivated by this sort of intuition, \n",
    "Sergey Ioffe and Christian Szegedy proposed [Batch Normalization](https://arxiv.org/abs/1502.03167),\n",
    "a technique that normalizes the mean and variance of each of the features at every level of representation during training. \n",
    "The technique involves normalization of the features across the examples in each mini-batch.\n",
    "While competing explanations for the technique's effect abound,\n",
    "its success is hard to deny.\n",
    "Empirically it appears to stabilize the gradient (less exploding or vanishing values)\n",
    "and batch-normalized models appear to overfit less.\n",
    "In fact, batch-normalized models seldom even use dropout. \n",
    "In this notebooks, we'll explain how it works.\n",
    "\n",
    "## Import dependencies and grab the MNIST dataset\n",
    "We'll get going by importing the typical packages and grabbing the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "mx.random.seed(1)\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization layer\n",
    "\n",
    "The layer, unlike Dropout, is usually used **before** the activation layer \n",
    "(according to the authors' original paper), instead of after activation layer.\n",
    "\n",
    "The basic idea is doing the normalization then applying a linear scale and shift to the mini-batch:\n",
    "\n",
    "For input mini-batch $B = \\{x_{1, ..., m}\\}$, we want to learn the parameter $\\gamma$ and $\\beta$.\n",
    "The output of the layer is $\\{y_i = BN_{\\gamma, \\beta}(x_i)\\}$, where:\n",
    "\n",
    "$$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m}x_i$$\n",
    "$$\\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(x_i - \\mu_B)^2$$\n",
    "$$\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "$$y_i \\leftarrow \\gamma \\hat{x_i} + \\beta \\equiv \\mbox{BN}_{\\gamma,\\beta}(x_i)$$\n",
    "\n",
    "* formulas taken from Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" International Conference on Machine Learning. 2015.\n",
    "\n",
    "With gluon, this is all actually implemented for us, \n",
    "but we'll do it this one time by ourselves,\n",
    "using the formulas from the original paper\n",
    "so you know how it works, and perhaps you can improve upon it!\n",
    "\n",
    "Pay attention that, when it comes to (2D) CNN, we normalize `batch_size * height * width` over each channel.\n",
    "So that `gamma` and `beta` have the lengths the same as `channel_count`.\n",
    "In our implementation, we need to manually reshape `gamma` and `beta` \n",
    "so that they could (be automatically broadcast and) multipy the matrices in the desired way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_batch_norm(X, gamma, beta, eps = 1e-5):\n",
    "    if len(X.shape) not in (2, 4):\n",
    "        raise ValueError('only supports dense or 2dconv')\n",
    "\n",
    "    # dense\n",
    "    if len(X.shape) == 2:\n",
    "        # mini-batch mean\n",
    "        mean = nd.mean(X, axis=0)\n",
    "        # mini-batch variance\n",
    "        variance = nd.mean((X - mean) ** 2, axis=0)\n",
    "        # normalize\n",
    "        X_hat = (X - mean) * 1.0 / nd.sqrt(variance + eps)\n",
    "        # scale and shift\n",
    "        out = gamma * X_hat + beta\n",
    "    \n",
    "    # 2d conv\n",
    "    elif len(X.shape) == 4:\n",
    "        # extract the dimensions\n",
    "        N, C, H, W = X.shape\n",
    "        # mini-batch mean\n",
    "        mean = nd.mean(X, axis=(0, 2, 3))\n",
    "        # mini-batch variance\n",
    "        variance = nd.mean((X - mean.reshape((1, C, 1, 1))) ** 2, axis=(0, 2, 3))\n",
    "        # normalize\n",
    "        X_hat = (X - mean.reshape((1, C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((1, C, 1, 1)) + eps)\n",
    "        # scale and shift\n",
    "        out = gamma.reshape((1, C, 1, 1)) * X_hat + beta.reshape((1, C, 1, 1))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some sanity checks. We expect each **column** of the input matrix to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nd.array([1,7,5,4,6,10], ctx=ctx).reshape((3,2))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_batch_norm(A,\n",
    "    gamma = nd.array([1,1], ctx=ctx), \n",
    "    beta=nd.array([0,0], ctx=ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = nd.array([1,1], ctx=ctx)\n",
    "be = nd.array([0,0], ctx=ctx)\n",
    "\n",
    "B = nd.array([1,6,5,7,4,3,2,5,6,3,2,4,5,3,2,5,6], ctx=ctx).reshape((2,2,2,2))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_batch_norm(B, ga, be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tests seem to support that we've done everything correctly.\n",
    "Note that for batch normalization, implementing **backward** pass is a little bit tricky. \n",
    "Fortunately, you won't have to worry about that here, \n",
    "because the MXNet's `autograd` package can handle differentiation for us automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides that, in the testing process, we want to use the mean and variance of the **complete dataset**, instead of those of **mini batches**. In the implementation, we use moving statistics as a trade off, because we don't want to or don't have the ability to compute the statistics of the complete dataset (in the second loop).\n",
    "\n",
    "Then here comes another concern: we need to maintain the moving statistics **along with multiple runs of the BN**. It's an engineering issue rather than a deep/machine learning issue. On the one hand, the moving statistics are similar to `gamma` and `beta`; on the other hand, they are **not** updated by the gradient backwards. In this quick-and-dirty implementation, we use the global dictionary variables to store the statistics, in which each key is the name of the layer (`scope_name`), and the value is the statistics. (**Attention**: always be very careful if you have to use global variables!) Moreover, we have another parameter `is_training` to indicate whether we are doing training or testing.\n",
    "\n",
    "Now we are ready to define our complete `batch_norm()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X,\n",
    "               gamma,\n",
    "               beta,\n",
    "               momentum = 0.9,\n",
    "               eps = 1e-5,\n",
    "               scope_name = '',\n",
    "               is_training = True,\n",
    "               debug = False):\n",
    "    \"\"\"compute the batch norm \"\"\"\n",
    "    global _BN_MOVING_MEANS, _BN_MOVING_VARS\n",
    "    \n",
    "    #########################\n",
    "    # the usual batch norm transformation\n",
    "    #########################\n",
    "    \n",
    "    if len(X.shape) not in (2, 4):\n",
    "        raise ValueError('the input data shape should be one of:\\n' + \n",
    "                         'dense: (batch size, # of features)\\n' + \n",
    "                         '2d conv: (batch size, # of features, height, width)'\n",
    "                        )\n",
    "    \n",
    "    # dense\n",
    "    if len(X.shape) == 2:\n",
    "        # mini-batch mean\n",
    "        mean = nd.mean(X, axis=0)\n",
    "        # mini-batch variance\n",
    "        variance = nd.mean((X - mean) ** 2, axis=0)\n",
    "        # normalize\n",
    "        if is_training:\n",
    "            # while training, we normalize the data using its mean and variance\n",
    "            X_hat = (X - mean) * 1.0 / nd.sqrt(variance + eps)\n",
    "        else:\n",
    "            # while testing, we normalize the data using the pre-computed mean and variance\n",
    "            X_hat = (X - _BN_MOVING_MEANS[scope_name]) *1.0 / nd.sqrt(_BN_MOVING_VARS[scope_name] + eps)\n",
    "        # scale and shift\n",
    "        out = gamma * X_hat + beta\n",
    "    \n",
    "    # 2d conv\n",
    "    elif len(X.shape) == 4:\n",
    "        # extract the dimensions\n",
    "        N, C, H, W = X.shape\n",
    "        # mini-batch mean\n",
    "        mean = nd.mean(X, axis=(0,2,3))\n",
    "        # mini-batch variance\n",
    "        variance = nd.mean((X - mean.reshape((1, C, 1, 1))) ** 2, axis=(0, 2, 3))\n",
    "        # normalize\n",
    "        X_hat = (X - mean.reshape((1, C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((1, C, 1, 1)) + eps)\n",
    "        if is_training:\n",
    "            # while training, we normalize the data using its mean and variance\n",
    "            X_hat = (X - mean.reshape((1, C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((1, C, 1, 1)) + eps)\n",
    "        else:\n",
    "            # while testing, we normalize the data using the pre-computed mean and variance\n",
    "            X_hat = (X - _BN_MOVING_MEANS[scope_name].reshape((1, C, 1, 1))) * 1.0 \\\n",
    "                / nd.sqrt(_BN_MOVING_VARS[scope_name].reshape((1, C, 1, 1)) + eps)\n",
    "        # scale and shift\n",
    "        out = gamma.reshape((1, C, 1, 1)) * X_hat + beta.reshape((1, C, 1, 1))\n",
    "      \n",
    "    #########################\n",
    "    # to keep the moving statistics\n",
    "    #########################\n",
    "    \n",
    "    # init the attributes\n",
    "    try: # to access them\n",
    "        _BN_MOVING_MEANS, _BN_MOVING_VARS\n",
    "    except: # error, create them\n",
    "        _BN_MOVING_MEANS, _BN_MOVING_VARS = {}, {}\n",
    "    \n",
    "    # store the moving statistics by their scope_names, inplace    \n",
    "    if scope_name not in _BN_MOVING_MEANS:\n",
    "        _BN_MOVING_MEANS[scope_name] = mean\n",
    "    else:\n",
    "        _BN_MOVING_MEANS[scope_name] = _BN_MOVING_MEANS[scope_name] * momentum + mean * (1.0 - momentum)\n",
    "    if scope_name not in _BN_MOVING_VARS:\n",
    "        _BN_MOVING_VARS[scope_name] = variance\n",
    "    else:\n",
    "        _BN_MOVING_VARS[scope_name] = _BN_MOVING_VARS[scope_name] * momentum + variance * (1.0 - momentum)\n",
    "        \n",
    "    #########################\n",
    "    # debug info\n",
    "    #########################\n",
    "    if debug:\n",
    "        print('== info start ==')\n",
    "        print('scope_name = {}'.format(scope_name))\n",
    "        print('mean = {}'.format(mean))\n",
    "        print('var = {}'.format(variance))\n",
    "        print('_BN_MOVING_MEANS = {}'.format(_BN_MOVING_MEANS[scope_name]))\n",
    "        print('_BN_MOVING_VARS = {}'.format(_BN_MOVING_VARS[scope_name]))\n",
    "        print('output = {}'.format(out))\n",
    "        print('== info end ==')\n",
    " \n",
    "    #########################\n",
    "    # return\n",
    "    #########################\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#  Set the scale for weight initialization and choose \n",
    "#  the number of hidden units in the fully-connected layer \n",
    "####################### \n",
    "weight_scale = .01\n",
    "num_fc = 128\n",
    "\n",
    "W1 = nd.random_normal(shape=(20, 1, 3,3), scale=weight_scale, ctx=ctx) \n",
    "b1 = nd.random_normal(shape=20, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "gamma1 = nd.random_normal(shape=20, loc=1, scale=weight_scale, ctx=ctx)\n",
    "beta1 = nd.random_normal(shape=20, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "W2 = nd.random_normal(shape=(50, 20, 5, 5), scale=weight_scale, ctx=ctx)\n",
    "b2 = nd.random_normal(shape=50, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "gamma2 = nd.random_normal(shape=50, loc=1, scale=weight_scale, ctx=ctx)\n",
    "beta2 = nd.random_normal(shape=50, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "W3 = nd.random_normal(shape=(800, num_fc), scale=weight_scale, ctx=ctx)\n",
    "b3 = nd.random_normal(shape=num_fc, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "gamma3 = nd.random_normal(shape=num_fc, loc=1, scale=weight_scale, ctx=ctx)\n",
    "beta3 = nd.random_normal(shape=num_fc, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "W4 = nd.random_normal(shape=(num_fc, num_outputs), scale=weight_scale, ctx=ctx)\n",
    "b4 = nd.random_normal(shape=10, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "params = [W1, b1, gamma1, beta1, W2, b2, gamma2, beta2, W3, b3, gamma3, beta3, W4, b4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    partition = nd.nansum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *softmax* cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "We insert the BN layer right after each linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, is_training = True, debug=False):\n",
    "    ########################\n",
    "    #  Define the computation of the first convolutional layer\n",
    "    ########################\n",
    "    h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=(3,3), num_filter=20)\n",
    "    h1_normed = batch_norm(h1_conv, gamma1, beta1, scope_name='bn1', is_training=is_training)\n",
    "    h1_activation = relu(h1_normed)\n",
    "    h1 = nd.Pooling(data=h1_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
    "    if debug:\n",
    "        print(\"h1 shape: %s\" % (np.array(h1.shape)))\n",
    "        \n",
    "    ########################\n",
    "    #  Define the computation of the second convolutional layer\n",
    "    ########################\n",
    "    h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=(5,5), num_filter=50)\n",
    "    h2_normed = batch_norm(h2_conv, gamma2, beta2, scope_name='bn2', is_training=is_training)\n",
    "    h2_activation = relu(h2_normed)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
    "    if debug:\n",
    "        print(\"h2 shape: %s\" % (np.array(h2.shape)))\n",
    "    \n",
    "    ########################\n",
    "    #  Flattening h2 so that we can feed it into a fully-connected layer\n",
    "    ########################\n",
    "    h2 = nd.flatten(h2)\n",
    "    if debug:\n",
    "        print(\"Flat h2 shape: %s\" % (np.array(h2.shape)))\n",
    "    \n",
    "    ########################\n",
    "    #  Define the computation of the third (fully-connected) layer\n",
    "    ########################\n",
    "    h3_linear = nd.dot(h2, W3) + b3\n",
    "    h3_normed = batch_norm(h3_linear, gamma3, beta3, scope_name='bn3', is_training=is_training)\n",
    "    h3 = relu(h3_normed)\n",
    "    if debug:\n",
    "        print(\"h3 shape: %s\" % (np.array(h3.shape)))\n",
    "        \n",
    "    ########################\n",
    "    #  Define the computation of the output layer\n",
    "    ########################\n",
    "    yhat_linear = nd.dot(h3, W4) + b4\n",
    "    if debug:\n",
    "        print(\"yhat_linear shape: %s\" % (np.array(yhat_linear.shape)))\n",
    "    \n",
    "    return yhat_linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run\n",
    "\n",
    "Can data be passed into the `net()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, _ in train_data:\n",
    "    data = data.as_in_context(ctx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = net(data, is_training=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        output = net(data, is_training=False) # attention here!\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Execute the training loop\n",
    "\n",
    "Note: you may want to use a gpu to run the code below. (And remember to set the `ctx = mx.gpu()` accordingly in the very beginning of this article.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "moving_loss = 0.\n",
    "learning_rate = .001\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, num_outputs)\n",
    "        with autograd.record():\n",
    "            # we are in training process,\n",
    "            # so we normalize the data using batch mean and variance\n",
    "            output = net(data, is_training=True)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(loss).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
    "            \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[Batch normalization with gluon](../chapter04_convolutional-neural-networks/cnn-batch-norm-gluon.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
