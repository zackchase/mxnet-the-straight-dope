{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Before we could begin writing,\n",
    "the authors of this book, \n",
    "like much of the work force, \n",
    "had to become caffeinated. \n",
    "We hopped in the car and started driving.\n",
    "Having an Android, Alex called out \"Okay Google\",\n",
    "awakening the phone's voice recognition system.\n",
    "Then Mu commanded \"directions to Blue Bottle coffee shop\".\n",
    "The phone quickly displayed the transcription of his command.\n",
    "It also recognized that we were asking for directions \n",
    "and launched the Maps application to fulfill our request. \n",
    "Once launched, the Maps app identified a number of routes. \n",
    "Next to each route, the phone displayed a predicted transit time.\n",
    "While we fabricated this story for pedagogical convenience,\n",
    "it demonstrates that in the span of just a few seconds, \n",
    "our everyday interactions with a smartphone\n",
    "can engage several machine learning models.\n",
    "\n",
    "\n",
    "If you've never worked with machine learning before,\n",
    "you might be wondering what the hell we're talking about. \n",
    "You might ask, \"Isn't that just programming?\"\n",
    "Or, \"What does *machine learning* even mean?\"\n",
    "First, to be clear, we implement all machine learning algorithms \n",
    "by writing computer programs. \n",
    "And we use many of the same languages and hardware \n",
    "as used in other fields of computer science.\n",
    "But not all computer programs involve machine learning. \n",
    "In response to the second question, \n",
    "precisely defining a field of study \n",
    "as vast as machine learning is hard. \n",
    "It's a bit like answering, \"what is math?\". \n",
    "But we'll try to give you enough intuition to get started.\n",
    "\n",
    "\n",
    "## A motivating example \n",
    "\n",
    "Most of the computer programs we interact with every day \n",
    "can be coded up from first principles.\n",
    "When you add an item to your shopping cart, \n",
    "you trigger an e-commerce application to store an entry \n",
    "in a *shopping cart* database table, \n",
    "associating your user ID with the product's ID. \n",
    "We can write such a program from first principles, \n",
    "launch without ever having seen a real customer.\n",
    "And when it's this easy to write an application \n",
    "*you should not be using machine learning*. \n",
    "\n",
    "But fortunately (for the community of ML scientists), \n",
    "for many problems, solutions aren't so easy.\n",
    "Returning to our fake story about going to get coffee,\n",
    "imagine just writing a program to respond to a *wake word* \n",
    "like \"Alexa\", \"Okay, Google\" or \"Siri\".\n",
    "Try coding it up in a room by yourself \n",
    "with nothing but a computer and a code editor. \n",
    "How would you write such a program from first principles?\n",
    "Think about it... the problem is hard.\n",
    "Every second, the microphone will collect roughly 44,000 samples.\n",
    "What rule could map reliably from a snippet of raw audio \n",
    "to confident predictions ``{yes, no}`` \n",
    "on whether the snippet contains the wake word?\n",
    "If you're stuck, don't worry. \n",
    "We don't know how to write such a program from scratch either. \n",
    "That's why we use machine learning. \n",
    "\n",
    "![](../img/wake-word.png)\n",
    "\n",
    "Here's the trick. \n",
    "Often, even when we don't know how to tell a computer \n",
    "explicitly how to map from inputs to outputs,\n",
    "we ourselves are nonetheless capable of performing the cognitive feat ourselves.\n",
    "In other words, even if you don't know *how to program a computer* to recognize the word \"Alexa\",\n",
    "you yourself *are able* to recognize the word \"Alexa\". \n",
    "Armed with this ability, \n",
    "we can collect a huge *data set* containing examples of audio\n",
    "and label those that *do* and that *do not* contain the wake word.\n",
    "In the machine learning approach, we do not design a system *explicitly* to recognize \n",
    "wake words right away. \n",
    "Instead, we define a flexible program with a number of *parameters*. \n",
    "These are knobs that we can tune to change the behavior of the program.\n",
    "We call this program a model.\n",
    "Generally, our model is just a machine that transforms its input into some output.\n",
    "In this case, the model receives as *input* a snippet of audio,\n",
    "and it generates as output an answer ``{yes, no}``,\n",
    "which we hope reflects whether (or not) the snippet contains the wake word.\n",
    "\n",
    "If we choose the right kind of model, \n",
    "then there should exist one setting of the knobs\n",
    "such that the model fires ``yes`` every time it hears the word \"Alexa\".\n",
    "There should also be another setting of the knobs that might fire ``yes`` \n",
    "on the word \"Apricot\". \n",
    "We expect that the same model should apply to \"Alexa\" recognition and \"Apricot\" recognition\n",
    "because these are similar tasks. \n",
    "However, we might need a different model to deal with fundamentally different inputs or outputs. \n",
    "For example, we might choose a different sort of machine to map from images to captions,\n",
    "or from English sentences to Chinese sentences. \n",
    "\n",
    "As you might guess, if we just set the knobs randomly,\n",
    "the model will probably recognize neither \"Alexa\", \"Apricot\",\n",
    "nor any other word in the English language. \n",
    "In most deep learning, the *learning* refers precisely \n",
    "to updating the model's behavior (by twisting the knobs)\n",
    "over the course of a *training period*. \n",
    "\n",
    "The training process usually looks like this:\n",
    "\n",
    "1. Start off with a randomly initialized model that can't do anything useful \n",
    "1. Grab some of your labeled data (e.g. audio snippets and corresponding ``{yes,no}`` labels)\n",
    "1. Tweak the knobs so the model sucks less with respect to those examples\n",
    "1. Repeat until the model is dope.\n",
    "\n",
    "![](../img/ml-loop.png)\n",
    "\n",
    "To summarize, rather than code up a wake word recognizer, \n",
    "we code up a program that, *when presented with a large labeled dataset*, \n",
    "can learn to recognize wake words. \n",
    "You can think of this act,\n",
    "of determining a program's behavior by presenting it with a dataset,\n",
    "as *programming with data*.\n",
    "\n",
    "\n",
    "## The dizzying versatility of machine learning\n",
    "\n",
    "This is the core idea behind machine learning:\n",
    "Rather than code programs with fixed behavior,\n",
    "we design programs with the ability to improve\n",
    "as they acquire more experience. \n",
    "This basic idea can take many forms.\n",
    "Machine learning can address many different application domains, \n",
    "involve many different types of models,\n",
    "and update them according to many different learning algorithms.\n",
    "In this particular case, we described an instance of *supervised learning* \n",
    "applied to a problem in automated speech recognition. \n",
    "\n",
    "Machine Learning is a versatile set of tools that lets you work with data in many different situations where simple rule-based systems would fail or might be very difficult to build. Due to its versatility, machine learning can be quite confusing to newcomers.\n",
    "For example, machine learning techniques are already widely used\n",
    "in applications as diverse as search engines, self driving cars, \n",
    "machine translation, medical diagnosis, spam filtering, \n",
    "game playing (*chess*, *go*), face recognition, \n",
    "data matching, calculating insurance premiums, and adding filters to photos. \n",
    "\n",
    "Despite the superficial differences between these problems many of them share a common structure\n",
    "and are addressable with deep learning tools. \n",
    "They're mostly similar because they are problems where we wouldn't be able to program their behavior directly in code, \n",
    "but we can *program them with data*.\n",
    "Often times the most direct language for communicating these kinds of programs is *math*. \n",
    "In this book, we'll introduce a minimal amount of mathematical notation,\n",
    "but unlike other books on machine learning and neural networks,\n",
    "we'll always keep the conversation grounded in real examples and real code.\n",
    "\n",
    "To make this conversation more concrete, let's consider a few examples and start writing some code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of machine learning\n",
    "When we considered the task of recognizing wake-words, \n",
    "we put together a dataset consisting of snippets and labels.\n",
    "We then described (albeit abstractly) \n",
    "how you might train a machine learning model\n",
    "to predict the label given a snippet. \n",
    "This set-up, predicting labels from examples, is just one flavor of ML \n",
    "and it's called *supervised learning*. \n",
    "Even within deep learning, there are many other approaches, \n",
    "and we'll discuss each in subsequent sections. \n",
    "To get going with at machine learning, we need four things: \n",
    "data, a model of how to transform the data,\n",
    "a loss function to measure how well we're doing, \n",
    "and an algorithm to tweak the model parameters \n",
    "such that the loss function is minimized. \n",
    "\n",
    "### Data \n",
    "\n",
    "Generally, the more data we have, the easier our job as modelers. When we have more data, we can train more powerful models. Data is at the heart of the resurgence of deep learning and many of most exciting models in deep learning don't work without large data sets. Here are some examples of the kinds of data machine learning practitioners often engage with:\n",
    "     \n",
    "* **Images:** Pictures taken by smartphones or harvested from the web, satellite images, photographs of medical conditions, ultrasounds, and radiologic images like CT scans and MRIs, etc. \n",
    "* **Text:** Emails, high school essays, tweets, news articles, doctor's notes, books, and corpora of translated sentences, etc.\n",
    "* **Audio:** Voice commands sent to smart devices like Amazon Echo, or iPhone or Android phones, audio books, phone calls, music recordings, etc.\n",
    "* **Video:** Television programs and movies, YouTube videos, cell phone footage, home surveillance, multi-camera tracking, etc.\n",
    "* **Structured data:** This Jupyter notebook (it contains text, images, code), webpages, electronic medical records, car rental records, electricity bills, etc.\n",
    "     \n",
    "### Models\n",
    "\n",
    "Usually, the data looks quite different from what we want to accomplish with it. For example, we might have photos of people and want to know whether they appear to be happy. So we might desire a model capable of ingesting a high-resolution image and outputting a happiness score. While some simple problems might be addressable with simple models, We're asking a lot in this case. To do its job, our happiness detector needs to transform hundreds of thousands of low-level features (pixel values) into something quite abstract on the other end (happiness scores). Choosing the right model is hard, and different models are better suited to different datasets. In this book, we'll be focusing mostly on deep neural networks. These models consist of many successive transformations of the data that are chained together top to bottom, thus the name *deep learning*. On our way to discussing deep nets, we'll also discuss some simpler, shallower models. \n",
    "\n",
    "* **Loss function.** To assess how well we're doing we need to compare the output from the model with the truth. Loss functions allow us to determine whether a stock prediction of \\$1,500 for ``AMZN`` by December 31, 2017 is correct. Depending on whether we decided to go short or long on it, we would incur different losses (or realize profits), hence our loss functions might be quite different. \n",
    "* **Training.** Usually, models have many parameters. These are the ones that we need to 'learn', by minimizing the loss incurred on training data. Unfortunately, doing well on the latter doesn't guarantee that we will do well on (unseen) test data, as the analogy below illustrates.\n",
    "     * **Training Error** - This is the error on the dataset used to find $f$ by minimizing the loss on the training set. This is equivalent to doing well on all the practice exams that a student might use to prepare for the real exam. Encouraging but by no means a guarantee.\n",
    "     * **Test Error** - This is the error incurred on an unseen test set. This can be off by quite a bit (statisticians call this overfitting). In real-life terms, this is the equivalent of screwing up the real exam despite doing well on the practice exams.\n",
    "  \n",
    "In the following sections, we will discuss a few types of machine learning in some more detail. This helps to understand what exactly one aims to do. We begin with a list of *objectives*, i.e. a list of things that machine learning can do. Note that the objectives are complemented with a set of techniques of *how* to accomplish them, i.e. training, types of data, etc. The list below is really only sufficient to whet the readers' appetite and to give us a common language when we talk about problems. We will introduce a larger number of such problems as we go along. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning \n",
    "\n",
    "Supervised learning describes the task of predicting targets $y$ \n",
    "given inputs $x$ by training on labeled examples. \n",
    "In probabilistic terms, supervised learning is concerned with estimating \n",
    "the conditional probability $P(y|x)$. \n",
    "And while it's just one among several approaches to machine learning, \n",
    "supervised learning accounts for the majority of machine learning in practice. \n",
    "Partly, that's because many important or valuable tasks \n",
    "can be described crisply as trying to predict something unknown (like a diagnosis),\n",
    "from something known (like symptoms). \n",
    "Predict cancer vs not cancer, given a CT image. \n",
    "Predict the correct translation in French, given a sentence in English.\n",
    "Predict the price of a stock next month based on this month's financial reporting data.\n",
    "\n",
    "Even with the simple description \"predict targets from inputs\" \n",
    "supervised learning can take a great many forms and require a great many modeling decisions,\n",
    "depending on the type, size, and the number of inputs and outputs. \n",
    "For example, we use different models to process sequences (like strings of text or time series data)\n",
    "and for processing fixed-length vector representations.\n",
    "We'll visit many of these problems in depth throughout the first 9 parts of this book. \n",
    "\n",
    "Put plainly, the learning process looks something like this.\n",
    "Grab a big pile of example inputs, selecting them randomly.\n",
    "Acquire the ground truth labels for each.\n",
    "Together, these inputs and corresponding labels (the desired outputs)\n",
    "comprise the training set. \n",
    "We feed the training dataset into a supervised learning algorithm.\n",
    "So here the *supervised learning algorithm* is a function that takes as input a dataset,\n",
    "and outputs another function, *the learned model*. \n",
    "Then, given a learned model, \n",
    "we can take a new previously unseen input, and predict the corresponding label.\n",
    "\n",
    "![](../img/supervised-learning.png)\n",
    "\n",
    "\n",
    "\n",
    "### Regression\n",
    "\n",
    "Perhaps the simplest supervised learning task to wrap your head around is Regression.\n",
    "Here, we have some input data consisting of numerical values, \n",
    "like the square footage of a house, the number of bedrooms and the number of bathrooms,\n",
    "and the number of minutes (walking) to the center of town. \n",
    "Formally we call such a collection of features a vector.\n",
    "If you live in New York or San Francisco and you are not the CEO of Amazon, Google, Microsoft, or Facebook, \n",
    "the (sqft, no. of bedrooms, no. bathrooms, walking distance) feature vector for your home \n",
    "might look something like: $[100, 0, .5, 60]$. \n",
    "However, if you live in Pittsburgh, \n",
    "it might look more like $[3000, 4, 3, 10]$.\n",
    "Feature vectors like this are essential for all the classic machine learning problems.\n",
    "We'll typically denote all the features for any one example (like a house) $\\mathbf{x_i}$\n",
    "and the set of feature vectors for all our examples $X$.\n",
    "\n",
    "What makes a problem *regression* is actually the outputs.\n",
    "Say that you're in the market for a new home, \n",
    "you might want to estimate the fair market value of a house,\n",
    "given some features like these. \n",
    "The target value, the price of sale, is a *real number*.\n",
    "We denote any individual target $y_i$ (corresponding to example $\\mathbf{x_i}$) \n",
    "and the set of all targets $\\mathbf{y}$ (corresponding to all examples X). \n",
    "When our targets take on arbitrary real values in some range, \n",
    "we call this a regression problem. \n",
    "The goal of our model is to produce predictions (guesses of the price, in our example)\n",
    "that closely approximate the actual target values.  \n",
    "We denote these predictions $\\hat{y}_i$ \n",
    "and if the notation seems whacky, then just ignore it for now. \n",
    "We'll unpack it more thoroughly in the subsequent chapters.\n",
    "\n",
    "\n",
    "Lots of practical problems are well-described regression problems. \n",
    "Predicting the rating that a user will assign to a movie is a regression problem.\n",
    "And if you designed a great algorithm to accomplish this feat in 2009,\n",
    "you might have won the [$1 million Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize).\n",
    "Predicting the length of stay for patients in the hospital is also a regression problem.\n",
    "A good rule of thumb is that any *How much?* or *How many?* problem should suggest regression.\n",
    "* \"How many hours will this surgery take?\"... *regression*\n",
    "* \"How many dogs are in this photo?\" ... *regression*.\n",
    "However, if you can easily pose your problem as \"Is this a ___?\", then it's likely, classification, a different fundamental problem type that we'll cover next.\n",
    "\n",
    "Even if you've never worked with machine learning before, \n",
    "you've probably worked through a regression problem informally. \n",
    "Imagine, for example, that you had your drains repaired \n",
    "and the contractor spent $x_1=3$ hours removing gunk from your sewage pipes,\n",
    "and then sent you a bill of $y_1 = 350\\$ $. \n",
    "Now imagine that your friend hired the same contractor for $x_2 = 2$ hours \n",
    "and that he received a bill of $y_2 = 250\\$ $. \n",
    "If someone then asked you how much to expect on their upcoming gunk-removal invoice\n",
    "you might make some reasonable assumptions (more hours $\\rightarrow$ more dollars)\n",
    "You might also assume that there's some base charge and that the contractor then charges per hour.\n",
    "If these assumptions held, then given these two data points, \n",
    "you could already identify the contractor's pricing structure: \n",
    "\\$100 per hour plus \\$50 to show up at your house. \n",
    "If you followed that much then you already understand the high-level idea behind linear regression.\n",
    "\n",
    "In this case, we could produce the parameters that exactly matched the contractor's prices. \n",
    "Sometimes that's not possible, e.g., if some of the variance owes to some factors besides your two features.\n",
    "In these cases, we'll try to learn models that minimize the distance between our predictions and the observed values.\n",
    "In most of our chapters, we'll focus on one of two very common losses, the [L1 loss](http://mxnet.io/api/python/gluon.html#mxnet.gluon.loss.L1Loss) where $l(y,y') = \\sum_i |y_i-y_i'|$ and the [L2 loss](http://mxnet.io/api/python/gluon.html#mxnet.gluon.loss.L2Loss) where $l(y,y') = \\sum_i (y_i - y_i')^2$.\n",
    "As we will see later, the $L_2$ loss corresponds to the assumption that our data was corrupted by Gaussian noise, whereas the $L_1$ loss corresponds to an assumption of noise from a Laplace distribution. \n",
    "\n",
    "### Classification\n",
    "\n",
    "While regression models are great for addressing *how many?* questions, lots of problems don't bend comfortably to this template. For example, say we wanted to build an automated system for optical character recognition (OCR). In other words, given an image of a hand-written character, we'd like to say which letter or number or punctuation mark is depicted. This kind of problem is called a classification and it's treated with a distinct set of algorithms. \n",
    "In classification, we want to look at a feature vector and then say which among a set of categories (formally called *classes*) an example belongs to.\n",
    "\n",
    "More formally, given example data $X$, such as images, text, sound, video, medical diagnostics, performance of a car, motion sensor data, etc., we want to answer the question as to which class $y \\in Y$ the data belongs to. The simplest form of classification is when there are only two classes, a problem which we call binary classification.\n",
    "For example, our dataset $X$ could consist of images of animals \n",
    "and our *labels* $Y$ might be the classes $\\mathrm{\\{cat, dog\\}}$.\n",
    "While in regression, we sought a regressor to output a real value $\\hat{y}$,\n",
    "in classification, we seek a *classifier*, whose output $\\hat{y}$ is the predicted class assignment.\n",
    "\n",
    "For reasons that we'll get into as the book gets more technical, it's pretty hard to optimize a model that can only output a hard categorical assignment, e.g. either *cat* or *dog*. \n",
    "It's a lot easier instead to express the model in the language of probabilities. \n",
    "Given an example $x$, the model assigns a probability $\\hat{y}_k$ to each label $k$. \n",
    "Because these are probabilities, they need to be positive numbers and add up to $1$. \n",
    "This means that we only need $K-1$ numbers to give the probabilities of $K$ categories.\n",
    "This is easy to see for binary classification. \n",
    "If there's a .6 probability that an unfair coin comes up heads, \n",
    "then there's a .4 probability that it comes up tails. \n",
    "Returning to our animal classification example, a classifier might see an image \n",
    "and output the probability that the image is a cat $\\Pr(y=\\mathrm{cat}\\mid x) = 0.9$.\n",
    "We can interpret this number by saying that the classifier is 90% sure that the image depicts a cat. \n",
    "The magnitude of the probability for the predicted class is one notion of confidence. \n",
    "It's not the only notion of confidence and we'll discuss different notions of uncertainty in more advanced chapters.\n",
    "\n",
    "When we have more than two possible classes, we call the problem *multiclass classification*.\n",
    "Common examples include hand-written character recognition `[0, 1, 2, 3 ... 9, a, b, c, ...]`. \n",
    "While we attacked regression problems by trying to minimize the L1 or L2 loss functions,\n",
    "the common loss function for classification problems is called cross-entropy.\n",
    "In `MXNet Gluon`, the corresponding loss function can be found [here](http://mxnet.io/api/python/gluon.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss). \n",
    "\n",
    "Note that the most likely class is not necessarily the one that you're going to use for your decision. Assume that you find this beautiful mushroom in your backyard:\n",
    "\n",
    "|![](../img/death_cap.jpg)|\n",
    "|:-------:|\n",
    "|Death cap - do not eat!|\n",
    "\n",
    "Now, assume that you built a classifier and trained it \n",
    "to predict if a mushroom is poisonous based on a photograph.\n",
    "Say our poison-detection classifier outputs $\\Pr(y=\\mathrm{death cap}\\mid\\mathrm{image}) = 0.2$. \n",
    "In other words, the classifier is rather confident that our mushroom *is not* a death cap. \n",
    "Still, you'd have to be a fool to eat it. \n",
    "That's because the certain benefit of a delicious dinner isn't worth a 20% chance of dying from it. \n",
    "In other words, the effect of the *uncertain risk* by far outweighs the benefit. \n",
    "Let's look at this in math. Basically, we need to compute the expected risk that we incur, i.e. we need to multiply the probability of the outcome with the benefit (or harm) associated with it:\n",
    "\n",
    "$$L(\\mathrm{action}\\mid x) = \\mathbf{E}_{y \\sim p(y\\mid x)}[\\mathrm{loss}(\\mathrm{action},y)]$$\n",
    "\n",
    "Hence, the loss $L$ incurred by eating the mushroom is $L(a=\\mathrm{eat}\\mid x) = 0.2 * \\infty + 0.8 * 0 = \\infty$, whereas the cost of discarding it is $L(a=\\mathrm{discard}\\mid x) = 0.2 * 0 + 0.8 * 1 = 0.8$. \n",
    "We got lucky - as any botanist would tell us, the above actually *is* a death cap.\n",
    "\n",
    "Classification can get much more complicated than just binary or even multiclass classification.\n",
    "For instance, there are some variants of classification for addressing hierarchies. \n",
    "Hierarchies assume that there exist some relationships among the many classes.\n",
    "So not all errors are equal - we prefer to misclassify to a related class than to a distant class.\n",
    "Usually, this is referred to as *hierarchical classification*. \n",
    "One early example of a hierarchy is due to Linnaeus, who applied the idea to animals. \n",
    "\n",
    "![](../img/taxonomy.jpg)\n",
    "So in this case, it might not be so bad to mistake a poodle for a schnauzer \n",
    "but our model would pay a huge penalty if it confused a poodle for a dinosaur. \n",
    "What hierarchy is relevant might depend on how you plan to use the model.\n",
    "For example, rattle snakes and garter snakes might be close on the phylogenetic tree, \n",
    "but mistaking a rattler for a garter could be deadly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging\n",
    "\n",
    "Some classification problems don't fit neatly into the binary or multiclass classification setups. \n",
    "For example, we could train a normal binary classifier to distinguish cats from dogs.\n",
    "And given the current state of computer vision, \n",
    "we can do this easily, with off-the-shelf tools.\n",
    "But no matter how accurate our model gets, we might find ourselves in trouble when the classifier encounters an image like this: \n",
    "\n",
    "![](../img/catdog.jpg)\n",
    "\n",
    "As you can see, there's a cat in the picture. And a dog. And a tire, some grass, a door, concrete, rust, individual grass leaves, etc. Depending on what we want to do with our model ultimately, treating this as a binary classification problem might not make a lot of sense. Instead, we might want to give the model the option of saying the image depicts a cat *and* a dog. Or neither a cat nor a dog. \n",
    "\n",
    "The problem of learning to predict classes that are *not mutually exclusive* is called multi-label classification. \n",
    "Auto-tagging problems are typically best described as multi-label classification problems. \n",
    "Think of the tags people might apply to posts on a tech blog, e.g. \"machine learning\", \"technology\", \"gadgets\", \"programming languages\", \"linux\", \"cloud computing\", \"AWS\". A typical article might have 5-10 tags applied because these concepts are correlated. Posts about \"cloud computing\" are likely to mention \"AWS\" and posts about \"machine learning\" could also deal with \"programming languages\". \n",
    "\n",
    "This problem emerges in the biomedical literature where correctly tagging articles is important because it allows researchers to do exhaustive reviews of the literature. At the National Library of Medicine, a number of professional annotators go over each article that gets indexed in PubMed to associate each with the relevant terms from MeSH, a collection of roughly 28k tags. This is a time-consuming process and the annotators typically have a one year lag between archiving and tagging. Machine learning can be used here to provide provisional tags until each article can have a proper manual review. Indeed, for several years, the BioASQ organization has [hosted a competition](http://bioasq.org/) to do precisely this.\n",
    "\n",
    "\n",
    "<!--\n",
    "The above example seems contrived but what about this case: a picture of a model posing in front of a car at the beach. Each of the tags `(woman, car, beach)` would be true. In other words, there are situations where we have multiple tags or attributes of what is contained in an object. Sometimes this is treated as a lot of binary classification problems. But this is problematic, too, since there are just so many tags (often hundreds of thousands or millions) that could apply, e.g. `(ham, green eggs, spam, grinch, ...)` and we would have to *check* all of them and to ensure that they are all accurate. \n",
    "\n",
    "Suffice it to say, there are better ways of generating tags. For instance, we could try to estimate the probability that $y$ is one of the tags in the set $S_x$ of tags associated with $x$, i.e. $\\Pr(y \\in S_x\\mid x)$. We will discuss them later in this tutorial (with actual code). For now just remember that *tagging is not classification*.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and ranking\n",
    "\n",
    "Sometimes we don't just want to assign each example to a bucket or to a real value. In the field of information retrieval, we want to impose a ranking on a set of items. Take web search for example, the goal is less to determine whether a particular page is relevant for a query, but rather, which one of the plethora of search results should be displayed for the user. We really care about the ordering of the relevant search results and our learning algorithm needs to produce ordered subsets of elements from a larger set. In other words, if we are asked to produce the first 5 letters from the alphabet, there is a difference between returning ``A B C D E`` and ``C A B E D``. Even if the result set is the same, the ordering within the set matters nonetheless.\n",
    "\n",
    "A possible solution to this problem is to score every element in the set of possible sets with a relevance score and then retrieve the top-rated elements. [PageRank](https://en.wikipedia.org/wiki/PageRank) is an early example of such a relevance score. One of the peculiarities is that it didn't depend on the actual query. Instead, it simply helped to order the results that contained the query terms. Nowadays search engines use machine learning and behavioral models to obtain query-dependent relevance scores. There are entire conferences devoted to this subject. \n",
    "\n",
    "<!-- Add / clean up-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Recommender systems\n",
    "\n",
    "Recommender systems are another problem setting that is related to search and ranking. The problems are  similar insofar as the goal is to display a set of relevant items to the user. The main difference is the emphasis on *personalization* to specific users in the context of recommender systems. For instance, for movie recommendations, the results page for a SciFi fan and the results page for a connoisseur of Woody Allen comedies might differ significantly. \n",
    "\n",
    "Such problems occur, e.g. for movie, product or music recommendation. In some cases, customers will provide explicit details about how much they liked the product (e.g. Amazon product reviews). In some other cases, they might simply provide feedback if they are dissatisfied with the result (skipping titles on a playlist). Generally, such systems strive to estimate some score $y_{ij}$ as a function of user $u_i$ and object $o_j$. The objects $o_j$ with the largest scores $y_{ij}$ are then used as a recommendation. Production systems are considerably more advanced and take detailed user activity and item characteristics into account when computing such scores. Below is an example of the books recommended for deep learning, based on the author's preferences.\n",
    "\n",
    "![](../img/deeplearning_amazon.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Learning\n",
    "\n",
    "So far we've looked at problems where we have some fixed number of inputs and produce a fixed number of outputs. \n",
    "Take some features of a home (square footage, number of bedrooms, number of bathrooms, walking time to downtown), and predict its value.\n",
    "Take an image (of fixed dimension) and produce a vector of probabilities (for a fixed number of classes).\n",
    "Take a user ID and a product ID and predict a star rating.\n",
    "And once we feed our fixed-length input into the model to generate an output, the model immediately forgets what it just saw. \n",
    "\n",
    "This might be fine if our inputs really all look the same -- or if successive inputs have nothing to do with each other. \n",
    "But what if we were dealing with video?\n",
    "Then our guess of what's going on in each frame\n",
    "could be much stronger if we take into account the previous frames in the image. \n",
    "Or what if we wanted a model that could ingest sentences in some source language and predict their translation in another language?\n",
    "\n",
    "And what if we wanted a model to monitor patients in the intensive care unit and to fire off an alert if their risk of death in the next 24 hours exceeded some modest threshold. We definitely wouldn't want this model to throw away everything it knows about the patient history each hour and just make its predictions based on the most recent measurements. \n",
    "\n",
    "Some of the more exciting applications of machine learning are sequence learning problems. They require a model either ingest sequences of inputs or to emit sequences of outputs (or both!).\n",
    "These latter problems are sometimes referred to as ``seq2seq`` problems. \n",
    "Language translation is a ``seq2seq`` problem. \n",
    "Transcribing text from spoken speech is also a ``seq2seq`` problem.\n",
    "While it is impossible to consider all types of sequence transformations, a number of special cases are worth mentioning:\n",
    "\n",
    "#### Tagging and Parsing\n",
    "\n",
    "This involves annotating a text sequence with attributes. In other words, the number of inputs and outputs is essentially the same. For instance, we might want to know where the verbs and subjects are, we might want to know which words are the named entities. In general, the goal is to decompose and annotate text $x$ based on structural and grammatical assumptions to get some annotation $y$. This sounds more complex than it actually is. Below is a very simple example of annotating a sentence with tags regarding which word refers to a named entity. \n",
    "\n",
    "|`Tom wants to have dinner in Washington with Sally.`|\n",
    "|:--|\n",
    "|`E   -     -  -    -      -  E          -    E`|\n",
    "\n",
    "#### Automatic Speech Recognition\n",
    "\n",
    "Here the input sequence $x$ is the sound of a speaker, and the output $y$ is the textual transcript of what the speaker said. The challenge is that there are many more audio frames (sound is typically sampled at 8kHz or 16kHz), i.e. there is no 1:1 correspondence between audio and text. In other words, this is a seq2seq problem where the output is much shorter than the input. \n",
    "\n",
    "|`----D----e--e--e-----p----------- L----ea-------r---------ni-----ng-----` |\n",
    "|:--------------|\n",
    "|![Deep Learning](../img/speech.jpg)|\n",
    "\n",
    "#### Text to Speech\n",
    "\n",
    "TTS is the inverse of Speech Recognition. That is, the input $x$ is text and the output $y$ is an audio file. There, the output is *much longer* than the input. While it is easy for *humans* to recognize a bad audio file, this isn't quite so trivial for computers. The challenge is that the audio output is way longer than the input sequence. \n",
    "\n",
    "#### Machine Translation\n",
    "\n",
    "Unlike in the previous cases where the order of the inputs was preserved, in machine translation, order inversion can be vital. In other words, while we are still converting one sequence into another, neither the number of inputs and outputs or the order of corresponding data points are assumed to be the same. Consider the following example which illustrates the obnoxious tendency of Germans (*Alex writing here*) to place the verbs at the end of sentences. \n",
    "\n",
    "|German |Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?|\n",
    "|:------|:---------|\n",
    "|English|Did you already check out this excellent tutorial?|\n",
    "|Wrong alignment |Did you yourself already this excellent tutorial looked-at?|\n",
    "\n",
    "A number of related problems exist. For instance, determining the order in which a user reads a webpage is a two-dimensional layout analysis problem. Likewise, for dialogue problems, we need to take world-knowledge and prior state into account. This is an active area of research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "All the examples so far are related to *Supervised Learning*, \n",
    "i.e. situations where we feed the model \n",
    "a bunch of examples and a bunch of *corresponding target values*.\n",
    "You could think of supervised learning as having an extremely specialized job and an extremely anal boss. \n",
    "The boss stands over your shoulder and tells you exactly what to do in every situation until you learn to map from situations to actions.\n",
    "Working for such a boss sounds pretty lame. \n",
    "But on the other hand, it's easy to please this boss. You just recognize the pattern as quickly as possible and imitate their actions. \n",
    "\n",
    "In a completely opposite way,\n",
    "it could be frustrating to work for a boss \n",
    "who has no idea what they want you to do.\n",
    "But if you're a data scientist, you better get used to it. \n",
    "The boss might just hand you a giant dump of data and tell you to *do some data science with it!*\n",
    "This sounds vague because it is. \n",
    "We call this class of problems *unsupervised learning*, \n",
    "and the type and number of questions we could ask \n",
    "is limited only by our creativity. \n",
    "We will address a number of unsupervised learning techniques in later chapters. To whet your appetite for now, we describe a few of the questions you might ask:\n",
    "\n",
    "* Can we find a small number of prototypes that accurately summarize the data? E.g. given a set of photos, can we group  them into landscape photos, pictures of dogs, babies, cats, mountain peaks, etc.? Likewise, given a collection of users (with their behavior), can we group them into users with similar behavior? This problem is typically known as **clustering**.\n",
    "* Can we find a small number of parameters that accurately capture the relevant properties of the data? E.g. the trajectories of a ball are quite well described by velocity, diameter, and mass of the ball. Tailors have developed a small number of parameters that describe human body shape fairly accurately for the purpose of fitting clothes. These problems are referred to as **subspace estimation** problems. If the dependence is linear, it is called **principal component analysis**.\n",
    "* Is there a representation of (arbitrary structured) objects in Euclidean space (i.e. the space of vectors in $\\mathbb{R}^n$) such that symbolic properties can be well matched? This is called **representation learning** and it is used, to describe entities and their relations such as Rome - Italy + France = Paris. \n",
    "* Is there a description of the root causes of much of the data that we observe? For instance, if we have demographic data about house prices, pollution, crime, location, education, salaries, etc., can we discover how they are related simply based on empirical data? The field of **directed graphical models** and **causality** deals with this.\n",
    "* An important and exciting recent development is **generative adversarial networks**. They are basically a procedural way of synthesizing data. The underlying statistical mechanisms are tests to check whether real and fake data are the same. We will devote a few notebooks to them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with an environment\n",
    "\n",
    "So far, we haven't discussed where data actually comes from, \n",
    "or what actually *happens* when a machine learning model generates an output. \n",
    "And that's because supervised learning and unsupervised learning\n",
    "do not address these issues in a very sophisticated way.\n",
    "In either case, we grab a big pile of data up front,\n",
    "then do our pattern recognition without ever interacting with the environment again.\n",
    "Because all of the learning takes place after the algorithm is disconnected from the environment,\n",
    "this is called *offline learning*.\n",
    "For supervised learning, the process looks like this:\n",
    "\n",
    "![](../img/data-collection.png)\n",
    "\n",
    "\n",
    "This simplicity of offline learning has its charms.\n",
    "It let's us worry about pattern recognition in isolation.\n",
    "But the downside is that this problem formulation is quite limiting.\n",
    "If you are more ambitious, or if you grew up reading Asimov's Robot Series,\n",
    "then you might imagine artificially intelligent bots capable not only of making predictions,\n",
    "but of taking actions in the world. \n",
    "In this case, we might think not just of a *model* but of an *agent*. \n",
    "And we need to think about choosing *actions*, not just making *predictions*.\n",
    "Moreover, unlike predictions, actions actualy impact the environment. \n",
    "So if we want to train an intelligent agent,\n",
    "we must account for the way its actions might \n",
    "impact the future observations of the agent. \n",
    "\n",
    "\n",
    "Considering the interaction with an environment open a whole set of new modeling questions, like\n",
    "\n",
    "* Does the environment remember what we did previously?\n",
    "* Does the environment wants to help us (e.g. a user reading text into a speech recognizer)?\n",
    "* Or is it out to beat us, i.e. an adversarial setting like spam filtering (against spammers) or playing a game (vs an opponent)?\n",
    "* Or does it have not  care (in most cases)?\n",
    "* Are the dynamics of the environment steady, or shifting over time?\n",
    "\n",
    "This last question raises the problem of covariate shift,\n",
    "(when training and test data are different). \n",
    "It's a problem that most of us have experienced when taking exams written by a lecturer,\n",
    "while the homeworks were composed by his TAs. \n",
    "We'll describe briefly describe reinforcement learning, and adversarial learning, \n",
    "two settings that explicitly consider interaction with an environment. \n",
    "\n",
    "\n",
    "### Reinforcement learning\n",
    "\n",
    "If you're interested in using machine learning to develop an agent that interacts with an environment and takes actions, the you're probably going to wind up focusing on reinforcement learning (RL). \n",
    "This might include applications to robotics, to dialogue systems, \n",
    "and even to developing AI for video games. \n",
    "Deep reinforcement learning (DRL), which applies deep neural networks\n",
    "to RL problems, has surged in popularity, \n",
    "following the breakthrough deep Q-network that beat humans at Atari games using only the visual input,\n",
    "and the AlphaGo program which dethroned the world champion at the board game Go.\n",
    "\n",
    "Reinforcment learning gives a very general statement of a problem,\n",
    "in which an agent interacts with an environment over a series of *time steps*.\n",
    "At each time step $t$, the agent receives some observation $o_t$ from the environment,\n",
    "and must choose an action $a_t$ which is then transmitted back to the environment. \n",
    "Finally, the agent receives a reward $r_t$ from the environment.\n",
    "The agent then receives a subseqeunt observation, and chooses a subsequent action, and so on.\n",
    "The behavior of an RL agent is governed by a *policy*.\n",
    "In short, a *policy* is just a fucction that maps from observations to actions.\n",
    "and goal of reinforcemnt learning is to produce a good policy.\n",
    "\n",
    "![](../img/rl-environment.png)\n",
    "\n",
    "It's hard to overstate the generality of the RL framework.\n",
    "For example, we can cast any supervised learning problem as an RL problem. \n",
    "Say we had a classification problem. \n",
    "We could create an RL agent with one *action* corresponding to each class. \n",
    "We could then create an environment which gave a reward \n",
    "that was exactly equal to the loss function from the original supervised problem.\n",
    "\n",
    "But RL can also address many problems that supervised learning cannot. \n",
    "For example, in supervised learning, we always expect\n",
    "that the training input comes associated with the correct label.\n",
    "But in RL, we don't assume that for each observation, the environment tells us the optimal action.\n",
    "In general, we just get some reward.\n",
    "Moreover, the environment may not even tell us which actions led to the reward. \n",
    "\n",
    "Consider for example the game of chess. \n",
    "The only real reward signal comes at the end of the game when we either win, which we might assign a reward of 1,\n",
    "or when we lose, which we could assign a reward of -1.\n",
    "So reinforcement learners must deal with the *credit assignment problem*.\n",
    "Same goes for an employee who gets a promotion on October 11. \n",
    "That promotion likely reflects a large number of well-chosen actions over the previous year.\n",
    "Getting more promotions in the future requires figuring out what actions along the way led to the promotion.\n",
    "\n",
    "Reinforcement learners may also have to deal with the problem of partial observability. \n",
    "That is, the current observation might not tell you everything about your current state. \n",
    "Say a cleaning robot found itself trapped in one of many identical closets in a house.\n",
    "Inferring the precise location (and thus state) of the robot, \n",
    "might require considering its previous observerations before entering the closet. \n",
    "\n",
    "Finally at any given point, reinforcement learners might know of one good policy. \n",
    "But there might be many other strategies that the agent has never tried. \n",
    "So the reinforcement learner must constantly choose \n",
    "whether to *exploit* the best currently-known strategy\n",
    "or to *explore* the space of policies, \n",
    "potentially giving up some short-run reward in exchange for knowledge.\n",
    "\n",
    "\n",
    "\n",
    "### MDPs, bandits, and friends\n",
    "\n",
    "The general reinforcement learning problem\n",
    "is a very general setting. \n",
    "Actions affect subsequent observations. \n",
    "Rewards are only observed corresponding to the chosen actions.\n",
    "The environment may be either fully or partially observed.\n",
    "Accounting for all this complexity at once may ask too much of researchers.\n",
    "Moreover not every practical problem exhibits all this complexity.\n",
    "So researchers have studied a number of *special cases* of reinforcement learning problems. \n",
    "\n",
    "When the environment is fully observed, we call the RL problem a Markov Decision Process (MDP).\n",
    "When the state does not depend on the previous actions, \n",
    "we the problem a contextual bandit problem. \n",
    "And when there is no state, just a set of available actions with initially unknown rewards,\n",
    "this problem is the classic multi-armed bandit problem. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When *not* to use machine learning\n",
    "\n",
    "Let's take a closer look at the idea of programming data\n",
    "by considering an interaction that [Joel Grus](http://joelgrus.com) reported experiencing in a [job interview](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/). The interviewer asked him to code up Fizz Buzz. This is a children's game where the players count from 1 to 100 and will say *'fizz'* whenever the number is divisible by 3, *'buzz'* whenever it is divisible by 5, and *'fizzbuzz'* whenever it satisfies both criteria. Otherwise, they will just state the number. It looks like this:\n",
    "\n",
    "```\n",
    "1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizzbuzz 16 ...\n",
    "```\n",
    "\n",
    "The conventional way to solve such a task is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizzbuzz 16 17 fizz 19 buzz fizz 22 23 fizz buzz 26 fizz 28 29 fizzbuzz 31 32 fizz 34 buzz fizz 37 38 fizz buzz 41 fizz 43 44 fizzbuzz 46 47 fizz 49 buzz fizz 52 53 fizz buzz 56 fizz 58 59 fizzbuzz 61 62 fizz 64 buzz fizz 67 68 fizz buzz 71 fizz 73 74 fizzbuzz 76 77 fizz 79 buzz fizz 82 83 fizz buzz 86 fizz 88 89 fizzbuzz 91 92 fizz 94 buzz fizz 97 98 fizz buzz\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(1, 101):\n",
    "    if i % 15 == 0:\n",
    "        res.append('fizzbuzz')\n",
    "    elif i % 3 == 0:\n",
    "        res.append('fizz')\n",
    "    elif i % 5 == 0:\n",
    "        res.append('buzz')\n",
    "    else:\n",
    "        res.append(str(i))\n",
    "print(' '.join(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say, this isn't very exciting if you're a good programmer. Joel proceeded to 'implement' this problem in Machine Learning instead. For that to succeed, he needed a number of pieces:\n",
    "\n",
    "* Data X ``[1, 2, 3, 4, ...]`` and labels Y ``['fizz', 'buzz', 'fizzbuzz', identity]`` \n",
    "* Training data, i.e. examples of what the system is supposed to do. Such as ``[(2, 2), (6, fizz), (15, fizzbuzz), (23, 23), (40, buzz)]``\n",
    "* Features that map the data into something that the computer can handle more easily, e.g. ``x -> [(x % 3), (x % 5), (x % 15)]``. This is optional but helps a lot if you have it. \n",
    "\n",
    "Armed with this, Joel wrote a classifier in TensorFlow ([code](https://github.com/joelgrus/fizz-buzz-tensorflow)). The interviewer was nonplussed ... and the classifier didn't have perfect accuracy.\n",
    "\n",
    "Quite obviously, this is silly. Why would you go through the trouble of replacing a few lines of Python with something much more complicated and error prone? However, there are many cases where a simple Python script simply does not exist, yet a 3-year-old child will solve the problem perfectly. \n",
    "\n",
    "|![](../img/cat1.jpg)|![](../img/cat2.jpg)|![](../img/dog1.jpg)|![](../img/dog2.jpg)|\n",
    "|:---------------:|:---------------:|:---------------:|:---------------:|\n",
    "|cat|cat|dog|dog|\n",
    "\n",
    "Fortunately, this is precisely where machine learning comes to the rescue. We can 'program' a cat detector by providing our machine learning system with many examples of cats and dogs. This way it will eventually learn a function that will e.g. emit a very large positive number if it's a cat, a very large negative number if it's a dog, and something closer to zero if it isn't sure. But this is just barely scratching the surface of what machine learning can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Machine Learning is vast. We cannot possibly cover it all. On the other hand, neural networks are simple and only require elementary mathematics. So let's get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[Manipulate data the MXNet way with NDArray](../chapter01_crashcourse/ndarray.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
