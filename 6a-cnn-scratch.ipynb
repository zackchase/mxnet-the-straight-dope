{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks from scratch\n",
    "\n",
    "[bloviate some minimal amount]\n",
    "[there's still a bug in this ish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data (last one, we promise!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = mx.test_utils.get_mnist()\n",
    "batch_size = 64\n",
    "train_data = mx.io.NDArrayIter(mnist[\"train_data\"], mnist[\"train_label\"], batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks (CNNs)\n",
    "\n",
    "In the [previous example](5a-mlp-scratch.ipynb), we built the simplest possible neural network. Every node in the each layer was connected to every node in the subsequent layers. \n",
    "\n",
    "![](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/img/multilayer-perceptron.png?raw=true![image.png](attachment:image.png)\n",
    "\n",
    "This can require a lot of parameters! If our input were a 256x256 color image (still quite small for a photograph), and our network had 1,000 nodes in the first hidden layer, then our first weight matrix would require (256x256x3)x1000 parameters. That's nearly 200 million. Moreover the hidden layer would toss aside all the spatial structure contained in the input image. Nearby pixels in the input exhibit some local structure but nearby nodes in our fully-connected layer would have no special relationship.\n",
    "\n",
    "Convolutional neural networks incorporate convolutional layers. These layers associate each of their nodes with a small window, called a *receptive field*, in the previous layer. \n",
    "\n",
    "![](http://cs231n.github.io/assets/cnn/depthcol.jpeg)\n",
    "(Image credit: Stanford cs231n http://cs231n.github.io/assets/cnn/depthcol.jpeg)\n",
    "\n",
    "In short, there are two new concepts you need to grep here. First, we'll be using *convolutional* layers. Second, we'll be interleaving them with *pooling* layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parameters\n",
    "\n",
    "Each node in convolutional layer is associated associated with a 3D block (height x width x channel) in the input tensor. Moreover, the convolutional layer itself has multiple output chanels. So the layer is parameterized by a 4 dimensional weight tensor, commonly called a *convolutional kernel*. \n",
    "\n",
    "The output tensor is produced by sliding the kernel across the input image skiping locations according to a pre-defined *stride* (but we'll just assume that to be 1 in this tutorial). Let's initialize some such kernels from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = nd.random_normal(shape=(20, 1, 3,3)) *.01\n",
    "b1 = nd.random_normal(shape=20) * .01\n",
    "\n",
    "W2 = nd.random_normal(shape=(50, 20, 5, 5)) *.01\n",
    "b2 = nd.random_normal(shape=50) * .01\n",
    "\n",
    "W3 = nd.random_normal(shape=(800,128)) *.01\n",
    "b3 = nd.random_normal(shape=128) *.01\n",
    "\n",
    "W4 = nd.random_normal(shape=(128,10)) *.01\n",
    "b4 = nd.random_normal(shape=10) *.01\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And assign space for gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolving with MXNet's NDArrray\n",
    "\n",
    "To write a convolution when using *raw MXNet*, we use the function ``nd.Convolution()``. This function takes a few important arguments: inputs (``data``), a 4D weight matrix (``weight``), a bias (``bias``), the shape of the kernel (``kernel``), and a number of filters (``num_filter``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20, 26, 26)\n"
     ]
    }
   ],
   "source": [
    "data = train_data.next().data[0].as_in_context(ctx)\n",
    "conv = nd.Convolution(data=data, weight=W1, bias=b1, kernel=(3,3), num_filter=20)\n",
    "print(conv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape. The number of examples (64) remains unchanged. The number of channels (also called *filters*) has increased to 20. And because the (3,3) kernel can only be applied in 26 different heights and widths (without the kernel busting over the image border), our output is 26,26. There are some weird padding tricks we can use when we want the input and output to have the same height and width dimensions, but we won't get into that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average pooling\n",
    "\n",
    "The other new component of this model is the pooling layer. Pooling gives us a way to downsample in the spatial dimensions. Early convnets typically used average pooling, but max pooling tends to give better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20, 13, 13)\n"
     ]
    }
   ],
   "source": [
    "pool = nd.Pooling(data=conv, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "print(pool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the batch and channel components of the shape are unchanged but that the height and width have been downsampled from (26,26) to (13,13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X,nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    partition =nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "Now we're ready to define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, debug=False):\n",
    "    h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=(3,3), num_filter=20)\n",
    "    h1_activation = relu(h1_conv)\n",
    "    h1 = nd.Pooling(data=h1_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
    "    if debug:\n",
    "        print(\"h1 shape: %s\" % (np.array(h1.shape)))\n",
    "        \n",
    "    h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=(5,5), num_filter=50)\n",
    "    h2_activation = relu(h2_conv)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
    "    if debug:\n",
    "        print(\"h2 shape: %s\" % (np.array(h2.shape)))\n",
    "    \n",
    "    ########################\n",
    "    #  Flattening h2 so that we can feed it into a fully-connected layer\n",
    "    ########################\n",
    "    h2 = nd.flatten(h2)\n",
    "    if debug:\n",
    "        print(\"Flat h2 shape: %s\" % (np.array(h2.shape)))\n",
    "    \n",
    "    \n",
    "    h3_linear = nd.dot(h2, W3) + b3\n",
    "    h3 = relu(h3_linear)\n",
    "    if debug:\n",
    "        print(\"h3 shape: %s\" % (np.array(h3.shape)))\n",
    "        \n",
    "    yhat_linear = nd.dot(h3, W4) + b4\n",
    "    yhat = softmax(yhat_linear)\n",
    "    if debug:\n",
    "        print(\"yhat shape: %s\" % (np.array(yhat.shape)))\n",
    "    \n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run\n",
    "\n",
    "We can now print out the shapes of the activations at each layer by using the debug flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 shape: [64 20 13 13]\n",
      "h2 shape: [64 50  4  4]\n",
      "Flat h2 shape: [ 64 800]\n",
      "h3 shape: [ 64 128]\n",
      "yhat shape: [64 10]\n"
     ]
    }
   ],
   "source": [
    "output = net(data, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.sum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    \n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        with autograd.record():\n",
    "            data = batch.data[0].as_in_context(ctx)\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "        \n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.0761822477841, Train_acc 0.976640981735, Test_acc 0.977806528662\n",
      "Epoch 1. Loss: 0.0312450892538, Train_acc 0.980829052511, Test_acc 0.977598342652\n",
      "Epoch 2. Loss: 0.020098715804, Train_acc 0.982876712329, Test_acc 0.981232690669\n",
      "Epoch 3. Loss: 0.0174940529287, Train_acc 0.984467751142, Test_acc 0.983072160081\n",
      "Epoch 4. Loss: 0.00930268655741, Train_acc 0.985850456621, Test_acc 0.984602297774\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        with autograd.record():\n",
    "            data = batch.data[0].as_in_context(ctx)\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "            loss = cross_entropy(output, label_one_hot)\n",
    "            loss.backward()\n",
    "        SGD(params, .01)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if i == 0:\n",
    "            moving_loss = np.mean(loss.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
    "            \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is nearly all the important ideas you'll need to start attacking problem in computer vision. Believe it or not, if you knew just the content in this tutorial 5 years ago, you could have sold a startup to a Fortune500 company for millions of dollars. Unfortunately, the world has gotten marginally more sophisticates, so we'll have to come up with some more sophisticated tutorials to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
