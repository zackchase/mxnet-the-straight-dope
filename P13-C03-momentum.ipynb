{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why momentum matters\n",
    "\n",
    "When one uses gradient descent to iteratively improve their parameter values, one might opt to incorporate a momentum coefficient in their training. The informal justification is that it makes the descent smoother (less \"oscillatory\") in multi-dimensional and stochastic settings, and thus training occurs faster. Let's see why this might be the case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oscillations in gradient descent\n",
    "\n",
    "First, we recall from the [previous chapter](./P13-C02-gd-and-sgd.ipynb) the formulation of gradient descent: for an objective function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ taking $\\mathbf{w} = [w_1, w_2, \\ldots, w_d]^\\top$ as input, we perform the update\n",
    "\n",
    "$$\\mathbf{w}_t := \\mathbf{w}_{t-1} - \\eta \\nabla f(\\mathbf{w}_{t}),$$\n",
    "\n",
    "where $\\nabla f(\\mathbf{w}_t)$ is the gradient (vector of partial derivatives at $\\mathbf{x}$) and $\\eta$ is the learning rate.\n",
    "\n",
    "**[TODO: continue here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Plot f(x,y) = x^2 + 5y^2 in 3D space\n",
    "# TODO: Plot f(x,y) as contour plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to find parameters $(w_1,w_2)$ that minimize the loss function $f(w_1,w_2)$. Let's see how gradient descent performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Plot oscillating arrows on the contour plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: explain observation]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The momentum update\n",
    "\n",
    "We now formulate the mathematical description of momentum for gradient descent. We replace the single update rule with the following two updates:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_{t} &:= \\gamma \\mathbf{v}_{t-1} - \\eta \\nabla f(\\mathbf{x}_t),\\\\\n",
    "\\mathbf{x}_{t} &:= \\mathbf{x}_{t-1} + \\mathbf{v}_{t}.\n",
    "\\end{align}\n",
    "\n",
    "Here, the vector subscripts denote time (e.g., the current iteration). We have a new **velocity** vector $\\mathbf{v}$ to keep track of, along with a new **momentum** hyperparameter $\\gamma \\in [0, 1)$ to set. When $\\gamma = 0$, we get regular gradient descent; otherwise, we are updating $\\mathbf{x}$ with a velocity vector $\\mathbf{v}$, which is a weighted sum of the previous velocity vector and the negative gradient.\n",
    "\n",
    "To see how this works over time, let's expand out the velocity term:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_{t} &:= \\gamma \\mathbf{v}_{t-1} - \\eta \\nabla f(\\mathbf{x}_t)\\\\\n",
    "&:= \\gamma (\\gamma \\mathbf{v}_{t-2} - \\eta \\nabla f(\\mathbf{x}_{t-1})) - \\eta \\nabla f(\\mathbf{x}_{t}) = \\gamma^2 \\mathbf{v}_{t-2} - \\eta [\\gamma \\nabla f(\\mathbf{x}_{t-1}) + \\nabla f(\\mathbf{x}_{t})]\\\\\n",
    "&:= \\gamma^2 (\\gamma \\mathbf{v}_{t-3} - \\eta \\nabla f(\\mathbf{x}_{t-2})) - \\eta [\\gamma \\nabla f(\\mathbf{x}_{t-1}) - \\nabla f(\\mathbf{x}_{t})] = \\gamma^3 \\mathbf{v}_{t-3} - \\eta [\\gamma^2\\nabla f(\\mathbf{x}_{t-2}) + \\gamma \\nabla f(\\mathbf{x}_{t-1}) + \\nabla f(\\mathbf{x}_{t})]\\\\\n",
    "&:= \\dotsb\\\\\n",
    "&:= \\gamma^t \\mathbf{v}_0 -\\eta \\sum_{k=0}^{t} \\gamma^k \\nabla f(\\mathbf{x}_{t-k}).\n",
    "\\end{align}\n",
    "\n",
    "This shows that the current velocity $\\mathbf{v}_{t}$ is an *exponential moving average* of the negative gradients (and whatever initial velocity $\\mathbf{v}_0$ one chooses). Intuitively, we see that the newest gradient matters most, but all the past gradients play weaker and weaker roles as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How momentum dampens oscillations\n",
    "\n",
    "**[TODO: continue here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Implement momentum update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Plot less oscillating arrows on the contour plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Explain momentum (inertia) analogy]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicalities\n",
    "\n",
    "**[TODO: expand]**\n",
    "\n",
    "One might simply opt for $\\mathbf{v}_0 = 0$. For the momentum parameter, practical default is $\\gamma = 0.9$, but this can be tuned.\n",
    "\n",
    "**[TODO: discuss bias correction?]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov momentum\n",
    "\n",
    "**[TODO: discuss this?]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "**[TODO: compare convergence rates between GD, Momentum, Nesterov momentum?]**\n",
    "\n",
    "For demonstrating the aforementioned gradient-based optimization algorithms, we use the regression problem in the [linear regression chapter](./P02-C01-linear-regression-scratch.ipynb) as a case study.\n",
    "\n",
    "First, we import related libraries, generate the synthetic data, and construct the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Reproduce linear regression; perform gradient, momentum, etc. and compare convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: discuss results]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
