{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with ``gluon``\n",
    "\n",
    "Now that we've implemented a whole neural network from scratch, using nothing but ``mx.ndarray`` and ``mxnet.autograd``, let's see how we can make the same model while doing a lot less work.\n",
    "\n",
    "Again, let's import some packages, this time adding ``mxnet.gluon`` to the list of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the context\n",
    "\n",
    "And let's also set a context where we'll do most of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset\n",
    "\n",
    "Again we'll look at the problem of linear regression and stick with the same synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = nd.random_normal(shape=(10000,2))\n",
    "y = 2* X[:,0] - 3.4 * X[:,1] + 4.2 + .01 * nd.random_normal(shape=(10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data iterator\n",
    "\n",
    "We'll stick with the ``NDArrayIter`` for handling out data batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = mx.io.NDArrayIter(X, y, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "Before we had to individual allocate our parameters and then compose them as a model. While it's good to know how to do things from scratch, with ``gluon``, we can usually just compose a network from predefined standard layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(1, in_units=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters\n",
    "\n",
    "Before we can do anything with this model we'll have to initialize the weights. *MXNet* provides a variety of common initializers in ``mxnet.init``. Note that we pass the initializer a context. That's how we tell ``gluon`` model where should to store our parameters. Once we start training deep nets, we'll generally want to keep parameters on one or more GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss\n",
    "\n",
    "Instead of writing our own loss function wer'e just going to call down to ``gluon.loss.L2Loss`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Instead of writing gradient descent from scratch every time, we can instantiate a ``gluon.Trainer``, passing it a dictionary of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training loop\n",
    "\n",
    "You might have notived that it was a bit more concise to express our model in ``gluon``. For example, we didn't have to individually allocate parameters, define our loss function, or implement stochastic gradient descent. The benefits of relying on ``gluon``'s abstractions will grow substantially once we start working with much more complext models. But once we have all the basic pieces in place, the training loop itself is quite similar to what we would do if implementing everything from scratch. \n",
    "\n",
    "To refresh your memory. For some number of ``epochs``, we'll make a complete pass over the dataset (``train_data``), grabbing one mini-batch of inputs and the corresponding ground-truth labels at a time. \n",
    "\n",
    "Then, for each batch, we'll go through the following ritual. To make it maximally ritualistic, we'll :\n",
    "* Generate predictions (``yhat``) and the loss (``loss``) by executing a forward pass through the network.\n",
    "* Calculate gradients by making a backwards pass through the network (``loss.backward()``). \n",
    "* Update the model parameters by invoking our SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0. Moving avg of loss: 0.550449523926\n",
      "Epoch 0, batch 500. Moving avg of loss: 0.0248766543482\n",
      "Epoch 0, batch 1000. Moving avg of loss: 0.000377714028904\n",
      "Epoch 0, batch 1500. Moving avg of loss: 0.000196770685601\n",
      "Epoch 0, batch 2000. Moving avg of loss: 0.000200642416255\n",
      "Epoch 1, batch 0. Moving avg of loss: 1.96520282771e-06\n",
      "Epoch 1, batch 500. Moving avg of loss: 0.000204079477443\n",
      "Epoch 1, batch 1000. Moving avg of loss: 0.000215603294103\n",
      "Epoch 1, batch 1500. Moving avg of loss: 0.000195705539767\n",
      "Epoch 1, batch 2000. Moving avg of loss: 0.000200635417732\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for e in range(epochs):\n",
    "    moving_loss = 0.\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx).reshape((-1,1))\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            mse = loss(output, label)\n",
    "        mse.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        #  Keep a moving average of the losses\n",
    "        moving_loss = .99 * moving_loss + .01 * nd.sum(mse).asscalar()\n",
    "    \n",
    "        if i % 500 == 0:\n",
    "            print(\"Epoch %s, batch %s. Moving avg of loss: %s\" % (e, i, moving_loss))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "As you can see, even for a simple eample like linear regression, ``gluon`` can help you to write quick, clean, clode. Next, we'll repeat this exercise for multilayer perceptrons, extending these lessons to deep neural networks and (comparatively) real datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
