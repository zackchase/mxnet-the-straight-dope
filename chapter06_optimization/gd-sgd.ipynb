{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent and stochastic gradient descent \n",
    "\n",
    "## Why gradient matters\n",
    "\n",
    "In previous chapters on training deep learning models, you may have encountered the routines of updating parameter values based on computing gradients of objective functions. We did so in the hope that the values of objective functions may be iteratively reduced. Indeed, such routines are crucial for optimization algorithms used in the model training. You may wonder, why can we rely on such gradients to update model parameters for optimization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent in one-dimensional domain\n",
    "\n",
    "To answer this question, we need to formalize the problem in mathematics. We have tried to keep the mathematical content to the minumum necessary to get a proper understanding. The emphasis in this chapter is on conveying the underlying concepts rather than on the mathematical rigour.\n",
    "\n",
    "\n",
    "Let us consider a simple scenario with the objective function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. Note that the domain of $f$ is in one-dimensional. According to its Taylor series expansion as shown in the [introduction chapter](./optimization-intro.ipynb), we have\n",
    "\n",
    "$$f(x + \\epsilon) \\approx f(x) + f'(x) \\epsilon.$$\n",
    "\n",
    "Substituting $\\epsilon$ with $-\\eta f'(x)$ where $\\eta$ is a constant, we have\n",
    "\n",
    "$$f(x - \\eta f'(x)) \\approx f(x) -  \\eta f'(x)^2.$$\n",
    "\n",
    "If $\\eta$ is set as a small positive value, we obtain\n",
    "\n",
    "$$f(x - \\eta f'(x)) \\leq f(x).$$\n",
    "\n",
    "In other words, updating $x$ as \n",
    "\n",
    "$$x := x - \\eta f'(x)$$ \n",
    "\n",
    "may reduce the value of $f(x)$ if its current derivative value $f'(x) \\neq 0$. Since the derivative $f'(x)$ is a special case of gradient in one-dimensional domain, the above update of $x$ is gradient descent in one-dimensional domain.\n",
    "\n",
    "The positive scalar $\\eta$ is called the learning rate or step size. Note that a larger learning rate increases the chance of overshooting the global minimum and oscillating. However, if the learning rate is too small, the convergence can be very slow. In practice, a proper learning rate is usually selected with experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent  in multi-dimensional domain\n",
    "\n",
    "We go on to illustrate the idea of gradient descent in multi-dimensional domain. Consider the objective function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ that takes any multi-dimensional vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_d]^\\top$ as the input. The gradient of $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is defined by the vector of partial derivatives as \n",
    "\n",
    "$$\\nabla_\\mathbf{x} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\\bigg]^\\top.$$\n",
    "\n",
    "We use the notation $\\nabla f(\\mathbf{x})$ and $\\nabla_\\mathbf{x} f(\\mathbf{x})$ exchangeably when there is no ambiguity.\n",
    "In plain English, each element $\\partial f(\\mathbf{x})/\\partial x_i$ of the gradient indicates the rate of change for $f$ at the point $\\mathbf{x}$ in correspondance to the increase of $x_i$ only. To measure the rate of change of $f$ in any direction that is represented by a unit vector $\\mathbf{u}$, in multivariate calculus we define the directional derivative of $f$ at $\\mathbf{x}$ in the direction of $\\mathbf{u}$ as\n",
    "\n",
    "$$D_\\mathbf{u} f(\\mathbf{x}) = \\lim_{h \\rightarrow 0}  \\frac{f(\\mathbf{x} + h \\mathbf{u}) - f(\\mathbf{x})}{h},$$\n",
    "\n",
    "which can be rewritten according to the chain rule as\n",
    "\n",
    "$$D_\\mathbf{u} f(\\mathbf{x}) = \\nabla f(\\mathbf{x}) \\cdot \\mathbf{u}.$$\n",
    "\n",
    "Since $D_\\mathbf{u} f(\\mathbf{x})$ gives the rates of change of $f$ at the point $\\mathbf{x}$ in all possible directions, to minimize $f$, we are interested in finding the direction where $f$ can be reduced fastest. Thus, we can minimize the directional derivative $D_\\mathbf{u} f(\\mathbf{x})$ with respect to $\\mathbf{u}$. Since $D_\\mathbf{u} f(\\mathbf{x}) = \\|\\nabla f(\\mathbf{x})\\| \\cdot \\|\\mathbf{u}\\|  \\cdot \\text{cos} (\\theta) = \\|\\nabla f(\\mathbf{x})\\|  \\cdot \\text{cos} (\\theta)$, where $\\theta$ is the angle between $\\nabla f(\\mathbf{x})$ and $\\mathbf{u}$, the minimum value of $\\text{cos}(\\theta)$ is -1 when $\\theta = \\pi$. Therefore, $D_\\mathbf{u} f(\\mathbf{x})$ is minimized when $\\mathbf{u}$ is at the opposite direction of the gradient $\\nabla f(\\mathbf{x})$. Now we can iteratively reduce the value of $f$ with the following gradient descent update:\n",
    "\n",
    "$$\\mathbf{x} := \\mathbf{x} - \\eta \\nabla f(\\mathbf{x}),$$\n",
    "\n",
    "where the positive scalar $\\eta$ is called the learning rate or step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "However, the gradient descent algorithm may be infeasible when the training data size is huge. Thus, a stochastic version of the algorithm is often used instead. \n",
    "\n",
    "To motivate the use of stochastic optimization algorithms, note that when training deep learning models, we often consider the objective function as a sum of a finite number of functions:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\mathbf{x}),$$\n",
    "\n",
    "where $f_i(\\mathbf{x})$ is a loss function based on the training data instance indexed by $i$. It is important to highlight that the per-iteration computational cost in gradient descent scales linearly with the training data set size $n$. Hence, when $n$ is huge, the per-iteration computational cost of gradient descent is very high.\n",
    "\n",
    "In view of this, stochastic gradient descent offers a lighter-weight solution. At each iteration, rather than computing the gradient $\\nabla f(\\mathbf{x})$, stochastic gradient descent randomly samples $i$ at uniform and computes $\\nabla f_i(\\mathbf{x})$ instead. The insight is, stochastic gradient descent uses $\\nabla f_i(\\mathbf{x})$ as an unbiased estimator of $\\nabla f(\\mathbf{x})$ since\n",
    "\n",
    "$$\\mathbb{E}_i \\nabla f_i(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x}).$$\n",
    "\n",
    "\n",
    "In a generalized case, at each iteration a mini-batch $\\mathcal{B}$ that consists of indices for training data instances may be sampled at uniform with replacement. \n",
    "Similarly, we can use \n",
    "\n",
    "$$\\nabla f_\\mathcal{B}(\\mathbf{x}) = \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}\\nabla f_i(\\mathbf{x})$$ \n",
    "\n",
    "to update $\\mathbf{x}$ as\n",
    "\n",
    "$$\\mathbf{x} := \\mathbf{x} - \\eta \\nabla f_\\mathcal{B}(\\mathbf{x}),$$\n",
    "\n",
    "where $|\\mathcal{B}|$ denotes the cardinality of the mini-batch and the positive scalar $\\eta$ is the learning rate or step size. Likewise, the mini-batch stochastic gradient $\\nabla f_\\mathcal{B}(\\mathbf{x})$ is an unbiased estimator for the gradient $\\nabla f(\\mathbf{x})$:\n",
    "\n",
    "$$\\mathbb{E}_\\mathcal{B} \\nabla f_\\mathcal{B}(\\mathbf{x}) = \\nabla f(\\mathbf{x}).$$\n",
    "\n",
    "This generalized stochastic algorithm is also called mini-batch stochastic gradient descent and we simply refer to them as stochastic gradient descent (as generalized). The per-iteration computational cost is $\\mathcal{O}(|\\mathcal{B}|)$. Thus, when the mini-batch size is small, the computational cost at each iteration is light.\n",
    "\n",
    "There are other practical reasons that may make stochastic gradient descent more appealing than gradient descent. If the training data set has many redundant data instances, stochastic gradients may be so close to the true gradient $\\nabla f(\\mathbf{x})$ that a small number of iterations will find useful solutions to the optimization problem. Besides, stochastic gradient descent can be considered as offering a regularization effect especially when the mini-batch size is small due to the randomness and noise in the mini-batch sampling. Moreover, certain hardware processes mini-batches of specific sizes more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "For demonstrating the aforementioned gradient-based optimization algorithms, we use the regression problem in the [linear regression chapter](../chapter02_supervised-learning/linear-regression-scratch.ipynb) as a case study.\n",
    "\n",
    "First, We import related libraries, generate the synthetic data, and construct the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(10000, 2)\n",
    "Y = 2 * X[:,0] - 3.4 * X[:,1] + 4.2 + .01 * np.random.normal(size=10000)\n",
    "\n",
    "ctx = mx.cpu()\n",
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(1))\n",
    "net.collect_params().initialize()\n",
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we specify the batch sizes and learning rates for stochastic gradient descent algorithms.\n",
    "Since the number of samples is 10,000, when the batch size is 10,000, the algorithm is essentially gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_sizes = [1, 10, 100, 1000, 10000]\n",
    "learning_rates = [0.1, 0.1, 0.5, 0.5, 1.0]\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the models and observe the inferred parameter values after the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1 dense0_weight [ 2.00191855 -3.40352154]\n",
      "Batch size: 1 dense0_bias 4.19731\n",
      "Batch size: 10 dense0_weight [ 1.99962676 -3.40070438]\n",
      "Batch size: 10 dense0_bias 4.20087\n",
      "Batch size: 100 dense0_weight [ 1.99993229 -3.40007687]\n",
      "Batch size: 100 dense0_bias 4.20093\n",
      "Batch size: 1000 dense0_weight [ 2.0000422  -3.40001583]\n",
      "Batch size: 1000 dense0_bias 4.20009\n",
      "Batch size: 10000 dense0_weight [ 2.00000167 -3.40008521]\n",
      "Batch size: 10000 dense0_bias 4.20016\n"
     ]
    }
   ],
   "source": [
    "for batch_size, learning_rate in zip(batch_sizes, learning_rates):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24),\n",
    "                                    ctx=ctx, force_reinit=True)\n",
    "    train_data = mx.io.NDArrayIter(X, Y, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_data.reset()\n",
    "        for i, batch in enumerate(train_data):\n",
    "            data = batch.data[0].as_in_context(ctx)\n",
    "            label = batch.label[0].as_in_context(ctx).reshape((-1, 1))\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                mse = loss(output, label)\n",
    "            mse.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "\n",
    "    for para_name, para_value in net.collect_params().items():\n",
    "        print(\"Batch size:\", batch_size, para_name,\n",
    "              para_value.data().asnumpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, all the investigated algorithms find the weight vector to be close to [2, -3.4] and the bias term to be close to 4.2 as specified in the synthetic data generation.\n",
    "\n",
    "Although the above demonstration uses a fixed learning rate for stochastic gradient descent, in practice a decaying learning rate is often needed. This is because the noise in random sampling does not vanish throughout the iterations. For gradient descent, the true gradient generally gets smaller until becomes the zero vector; thus, it can use a fixed learning rate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
