{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Taste of Machine Learning\n",
    "\n",
    "Machine Learning is this incredibly versatile set of tools that lets you work with data in a way where simple rule-based systems would fail or be very difficult to build. Due to its versatility, machine learning can seem quite confusing in its manifestations (search engines, self driving cars, machine translation, medical diagnosis, spam filtering, chess and go playing, face recognition, finding a date, calculating insurance premiums, fancy filters for photos, etc.). This is largely due to the fact that machine learning lets you *program with data* rather than having to write code to accomplish a task. \n",
    "\n",
    "The language to express such things is *math*. We'll keep the latter to a minimum and focus mostly on code. That said, it's probably a good idea to know what a derivative (aka gradient) is, and what a matrix-vector product does. We'll be gentle and explain it as we go along. To get a better idea of what machine learning can do, let's consider a few examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming with Code vs. Programming with Data\n",
    "\n",
    "This example is inspired by an interaction that [Joel Grus](http://joelgrus.com) had in a [job interview](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/). The interviewer asked him to code up Fizz Buzz. This is a children's game where the players count from 1 to 100 and will say *'fizz'* whenever the number is divisible by 3, *'buzz'* whenever it is divisible by 5, and *'fizz buzz'* whenever it satisfies both criteria. Otherwise they will just state the number. It looks like this:\n",
    "\n",
    "``1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizz buzz 16 ...``\n",
    "\n",
    "The conventional way to solve such a task is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizzbuzz 16 17 fizz 19 buzz fizz 22 23 fizz buzz 26 fizz 28 29 fizzbuzz 31 32 fizz 34 buzz fizz 37 38 fizz buzz 41 fizz 43 44 fizzbuzz 46 47 fizz 49 buzz fizz 52 53 fizz buzz 56 fizz 58 59 fizzbuzz 61 62 fizz 64 buzz fizz 67 68 fizz buzz 71 fizz 73 74 fizzbuzz 76 77 fizz 79 buzz fizz 82 83 fizz buzz 86 fizz 88 89 fizzbuzz 91 92 fizz 94 buzz fizz 97 98 fizz buzz "
     ]
    }
   ],
   "source": [
    "for i in range(1,101):\n",
    "    if (i % 3) == 0:\n",
    "        print('fizz', end='')\n",
    "    if (i % 5) == 0:\n",
    "        print('buzz', end='')\n",
    "    if (i % 3) * (i % 5):\n",
    "        print(i, end='')\n",
    "    print(' ', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say, this isn't very exciting if you're a good programmer. Joel proceeded to 'implement' this problem in Machine Learning instead. For that to succeed, he needed a number of pieces:\n",
    "\n",
    "* Data X ``[1, 2, 3, 4, ...]`` and labels Y ``['fizz', 'buzz', 'fizzbuzz', identity]`` \n",
    "* Training data, i.e. examples of what the system is supposed to do. Such as ``[(2, 2), (6, fizz), (15, fizzbuzz), (23, 23), (40, buzz)]``\n",
    "* Features the map the data into something that the computer can handle more easily, e.g. ``x -> [(x % 3), (x % 5), (x % 15)]``. This is optional but helps a lot if you have it. \n",
    "\n",
    "Armed with this, Joel wrote a classifier in TensorFlow ([code](https://github.com/joelgrus/fizz-buzz-tensorflow)). The interviewer was nonplussed ... and the classifier didn't have perfect accuracy.\n",
    "\n",
    "Quite obviously, this is silly. Why would you go through the trouble of replacing a few lines of Python with something much more complicated and error prone. However, there are many cases where a simple Python script simply does not exist, yet a 3 year old child will solve the problem perfectly. \n",
    "<br>\n",
    "<img align=\"left\" src=\"img/cat1.jpg\" style=\"width: 25%;\"/> \n",
    "<img align=\"left\" src=\"img/cat2.jpg\" style=\"width: 25%;\"/> \n",
    "<img align=\"left\" src=\"img/dog1.jpg\" style=\"width: 25%;\"/> \n",
    "<img align=\"left\" src=\"img/dog2.jpg\" style=\"width: 25%;\"/>\n",
    "<br>\n",
    "\n",
    "Fortunately, this is precisely where machine learning comes to the rescue. We can 'program' a cat detector by providing our machine learning system with many examples of cats and dogs. This way it will eventually learn a function that will e.g. emit a very large positive number if it's a cat, a very large negative number if it's a dog, and something closer to zero if it isn't sure. But this is just barely scratching the surface of what machine learning can do ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "This is one of the simplest tasks. Given data $x \\in X$, such as images, text, sound, video, medical diagnostics, performance of a car, motion sensor data, etc., we want to answer the question as to which class $y \\in Y$ the data belongs to. In the above case, $X$ are images and $Y = \\mathrm{\\{cat, dog\\}}$. Quite often the confidence of the classifier, i.e. the algorithm that does this, is expressed in the form of probabilities, e.g. $\\Pr(y=\\mathrm{cat}|x) = 0.9$, i.e. the classifier is 90% sure that it's a cat. Whenever we have only two possible outcomes, statisticians call this a *binary classifier*. All other cases are called *multiclass classification*, e.g. the digits ``[0, 1, 2, 3 ... 9]`` in a digit recognition task. In ``MxNet Gluon`` the corresponding loss function is the [Cross Entropy Loss](http://mxnet.io/api/python/gluon.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss). \n",
    "\n",
    "Note that the most likely class is not necessarily the one that you're going to use for your decision. Assume that you find this beautiful mushroom in your backyard:\n",
    "![death cap](img/death_cap.jpg)\n",
    "\n",
    "Our (admittedly quite foolish) classifier outputs $\\Pr(y=\\mathrm{death cap}|\\mathrm{image}) = 0.2$. In other words, it is quite confident that it *isn't* a death cap. Nonetheless, you shouldn't eat it, since the cost of eating it (you die) is much higher than the cost of discarding it (you miss a tasty meal). Let's check how this looks like in math:\n",
    "$$L(a|x) = \\mathbf{E}_{y \\sim p(y|x)}[\\mathrm{loss}(a,y)]$$\n",
    "Hence, the loss $L$ incurred by eating the mushroom is $L(a=\\mathrm{eat}|x) = 0.2 * \\infty + 0.8 * 0 = \\infty$, whereas the cost of discarding it is $L(a=\\mathrm{discard}|x) = 0.2 * 0 + 0.8 * 1 = 0.8$. We got lucky - as any botanist would tell us, the above actually *is* a death cap.\n",
    "\n",
    "There are way more fancy classification problems than the ones above. For instance, we might have hierarchies. One of the first examples of such a thing are due to Linnaeus, who applied it to animals. Usually this is referred to as *hierarchical classification*. Typically the cost of misclassification will depend on how far you've strayed from the truth, e.g. mistaking a poodle for a schnautzer is no big deal but mistaking it for a dinosaur would be embarrassing. On the other hand, mistaking a rattle snake for a garter snake could be deadly. In other words, the cost might be *nonuniform* over the hierarchy of classes but tends to increase the further away you are from the truth. \n",
    "\n",
    "![taxonomy](img/taxonomy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging\n",
    "\n",
    "It is worth noting that many problems are *not* classification problems. Discerning cats and dogs by computer vision is relatively easy, but what should our poor classifier do in this situation?\n",
    "\n",
    "![catdog](img/catdog.jpg)\n",
    "\n",
    "Obviously there's a cat in the picture. And a dog. And a tire, some grass, a door, concrete, rust, individual grass leaves, etc.; Treating it as a binary classification problem is asking for trouble. Our poor classifier will get horribly confused if it needs to decide whether the image is one of two things, if it is actually both. \n",
    "\n",
    "The above example seems contrived but what about this case: a picture of a model posing in front of a car at the beach. Each of the tags ``(woman, car, beach)`` would be true. In other words, there are situations where we have multiple tags or attributes of what is contained in an object. Sometimes this is treated as a lot of binary classification problems. But this is problematic, too, since there are just so many tags (often hundreds of thousands or millions) that could apply, e.g. ``(ham, green eggs, spam, grinch, ...)`` and we would have to *check* all of them and to ensure that they are all accurate. \n",
    "\n",
    "Suffice it to say, there are better ways of generating tags. For instance, we could try to estimate the probability that $y$ is one of the tags in the set $S_x$ of tags associated with $x$, i.e. $\\Pr(y \\in S_x|x)$. We will discuss them later in this tutorial (with actual code). For now just remember that *tagging is not classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Let's assume that you're having your drains repaired and the contractor spends $x=3$ hours removing gunk from your sewage pipes. He then sends you a bill of $y = 350\\$ $. Your friend hires the same contractor for $x = 2$ hours and he gets a bill of $y = 250\\$ $. You can now both team up and perform a regression estimate to identify the contractor's pricing structure: \\$100 per hour plus \\$50 to show up at your house. That is, $f(x) = 100 \\cdot x + 50$.\n",
    "\n",
    "More generally, in regression we aim to obtain a real-valued number $y \\in \\mathbb{R}$ based on data $x$. Here $x$ could be as simple as the number of hours worked, or as complex as last week's news if we want to estimate the gain in a share price. For most of the tutorial, we will be using one of two very common losses, the [$L_1$ loss](http://mxnet.io/api/python/gluon.html#mxnet.gluon.loss.L1Loss) where $l(y,y') = \\sum_i |y_i-y_i'|$ and the [$L_2$ loss](http://mxnet.io/api/python/gluon.html#mxnet.gluon.loss.L2Loss) where $l(y,y') = \\sum_i (y_i - y_i')^2$. As we will see later, the $L_2$ loss corresponds to the assumption that our data was corrupted by Gaussian Noise, whereas the $L_1$ loss is very robust to malicious data corruption, albeit at the expense of lower efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and Ranking\n",
    "\n",
    "One of the problems quite different from classifications is ranking. There the goal is less to determine whether a particular page is relevant for a query, but rather, which one of the plethora of search results should be displayed for the user. That is, we really care about the ordering among the relevant search results and our learning algorithm needs to produce ordered subsets of elements from a larger set. In other words, if we are asked to produce the first 5 letters from the alphabet, there is a difference between returning ``A B C D E`` and ``C A B E D``. Even if the result set is the same, the ordering within the set matters nonetheless.\n",
    "\n",
    "A possible solution to this problem is to score every element in the set of possible sets with a relevance score and then retrieve the top-rated elements. [PageRank](https://en.wikipedia.org/wiki/PageRank) is an early example of such a relevance score. One of the peculiarities is that it didn't depend on the actual query. Instead, it simply helped to order the results that contained the query terms. Nowadays search engines use machine learning and behavioral models to obtain relevance scores. \n",
    "\n",
    "<br>\n",
    "<img align=\"left\" src=\"img/mxnet_google.png\" style=\"width: 80%;\"/> \n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recommender Systems\n",
    "\n",
    "Quite closely related to search and ranking are recommender systems. The problems are  similar insofar as the goal is to display a set of relevant items to the user. The main difference is the emphasis on *personalization* to specific users in the context of recommender systems. For instance, for movie recommendation the result page for a SciFi fan and the result page for a connoissoir of Woody Allen comedies might differ significantly. \n",
    "\n",
    "Such problems occur, e.g. for movie, product or music recommendation. In some cases customers will provide explicit details about how much they liked the product (e.g. Amazon product reviews). In some other cases they might simply provide feedback if they are dissatisfied with the result (skipping titles on a playlist). Generally, such systems strive to estimate some score $y_{ij}$ as a function of user $u_i$ and object $o_j$. The objects $o_j$ with the largest scores $y_{ij}$ are then used as recommendation. Production systems are considerably more advanced and take detailed user activity and item characteristics into account when computing such scores. Below an example of the books recommended for deep learning, based on the author's preferences.\n",
    "\n",
    "<br>\n",
    "<img align=\"left\" src=\"img/deeplearning_amazon.png\" style=\"width: 80%;\"/> \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Transformations\n",
    "\n",
    "Some of the more exciting applications of machine learning are sequence transformations, sometimes also referred as ``seq2seq`` problems. They ingest a sequence of data and emit a new, significantly transformed one. There are many use cases:\n",
    "\n",
    "* **Automatic Speech Recognition.** Here the input sequence $x$ is the sound of a speaker, and the output $y$ is the textual transcript of what the speaker said. \n",
    "\n",
    "|D--e--e--e-----p----------- L----ea----r---------ni-----ng----- |\n",
    "|--------------|\n",
    "|![Deep Learning](img/speech.jpg)|\n",
    "\n",
    "* **Machine Translation.**\n",
    "* **Text to Speech.**\n",
    "* **Named Entity Tagging.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The categorical distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
